{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2. Finding and Wrangling Time Series Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where to Find Time Series Data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepared Data Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The UCI Machine Learning Repository : https://archive.ics.uci.edu/ml/index.php\n",
    "\n",
    "The UEA and UCR Time Series Classification Repository: http://www.timeseriesclassification.com/\n",
    "\n",
    "Government time series data sets: \n",
    "\n",
    "https://www.ncdc.noaa.gov/cdo-web/datasets\n",
    "\n",
    "https://www.bls.gov/\n",
    "\n",
    "https://fred.stlouisfed.org/\n",
    "\n",
    "https://www.cdc.gov/flu/weekly/fluviewinteractive.htm\n",
    "\n",
    "Additional:\n",
    "\n",
    "https://www.comp-engine.org/\n",
    "\n",
    "https://cran.r-project.org/web/packages/Mcomp/index.html\n",
    "\n",
    "https://github.com/carlanetto/M4comp2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Worked Example: Assembling a Time Series Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "emails = pd.read_csv('BookRepo-master/Ch02/data/emails.csv')\n",
    "YearJoined = pd.read_csv('BookRepo-master/Ch02/data/year_joined.csv')\n",
    "donations =  pd.read_csv('BookRepo-master/Ch02/data/donations.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user</th>\n",
       "      <th>userStats</th>\n",
       "      <th>yearJoined</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>silver</td>\n",
       "      <td>2014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user userStats  yearJoined\n",
       "0     0    silver        2014"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "YearJoined.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>yearJoined</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>userStats</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           yearJoined\n",
       "userStats            \n",
       "1                1000"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## python\n",
    "YearJoined.groupby('user').count().groupby('userStats').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emailsOpened</th>\n",
       "      <th>user</th>\n",
       "      <th>week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2015-06-29 00:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   emailsOpened  user                 week\n",
       "0           3.0   1.0  2015-06-29 00:00:00"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emails.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emailsOpened</th>\n",
       "      <th>user</th>\n",
       "      <th>week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [emailsOpened, user, week]\n",
       "Index: []"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## python\n",
    "emails[emails.emailsOpened < 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>emailsOpened</th>\n",
       "      <th>user</th>\n",
       "      <th>week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25464</th>\n",
       "      <td>1.0</td>\n",
       "      <td>998.0</td>\n",
       "      <td>2017-12-04 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25465</th>\n",
       "      <td>3.0</td>\n",
       "      <td>998.0</td>\n",
       "      <td>2017-12-11 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25466</th>\n",
       "      <td>3.0</td>\n",
       "      <td>998.0</td>\n",
       "      <td>2017-12-18 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25467</th>\n",
       "      <td>3.0</td>\n",
       "      <td>998.0</td>\n",
       "      <td>2018-01-01 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25468</th>\n",
       "      <td>3.0</td>\n",
       "      <td>998.0</td>\n",
       "      <td>2018-01-08 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25469</th>\n",
       "      <td>2.0</td>\n",
       "      <td>998.0</td>\n",
       "      <td>2018-01-15 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25470</th>\n",
       "      <td>3.0</td>\n",
       "      <td>998.0</td>\n",
       "      <td>2018-01-22 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25471</th>\n",
       "      <td>2.0</td>\n",
       "      <td>998.0</td>\n",
       "      <td>2018-01-29 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25472</th>\n",
       "      <td>3.0</td>\n",
       "      <td>998.0</td>\n",
       "      <td>2018-02-05 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25473</th>\n",
       "      <td>3.0</td>\n",
       "      <td>998.0</td>\n",
       "      <td>2018-02-12 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25474</th>\n",
       "      <td>3.0</td>\n",
       "      <td>998.0</td>\n",
       "      <td>2018-02-19 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25475</th>\n",
       "      <td>2.0</td>\n",
       "      <td>998.0</td>\n",
       "      <td>2018-02-26 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25476</th>\n",
       "      <td>2.0</td>\n",
       "      <td>998.0</td>\n",
       "      <td>2018-03-05 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25477</th>\n",
       "      <td>3.0</td>\n",
       "      <td>998.0</td>\n",
       "      <td>2018-03-12 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25478</th>\n",
       "      <td>2.0</td>\n",
       "      <td>998.0</td>\n",
       "      <td>2018-03-19 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25479</th>\n",
       "      <td>2.0</td>\n",
       "      <td>998.0</td>\n",
       "      <td>2018-03-26 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25480</th>\n",
       "      <td>3.0</td>\n",
       "      <td>998.0</td>\n",
       "      <td>2018-04-02 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25481</th>\n",
       "      <td>3.0</td>\n",
       "      <td>998.0</td>\n",
       "      <td>2018-04-09 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25482</th>\n",
       "      <td>3.0</td>\n",
       "      <td>998.0</td>\n",
       "      <td>2018-04-16 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25483</th>\n",
       "      <td>3.0</td>\n",
       "      <td>998.0</td>\n",
       "      <td>2018-04-30 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25484</th>\n",
       "      <td>3.0</td>\n",
       "      <td>998.0</td>\n",
       "      <td>2018-05-07 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25485</th>\n",
       "      <td>3.0</td>\n",
       "      <td>998.0</td>\n",
       "      <td>2018-05-14 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25486</th>\n",
       "      <td>3.0</td>\n",
       "      <td>998.0</td>\n",
       "      <td>2018-05-21 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25487</th>\n",
       "      <td>3.0</td>\n",
       "      <td>998.0</td>\n",
       "      <td>2018-05-28 00:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       emailsOpened   user                 week\n",
       "25464           1.0  998.0  2017-12-04 00:00:00\n",
       "25465           3.0  998.0  2017-12-11 00:00:00\n",
       "25466           3.0  998.0  2017-12-18 00:00:00\n",
       "25467           3.0  998.0  2018-01-01 00:00:00\n",
       "25468           3.0  998.0  2018-01-08 00:00:00\n",
       "25469           2.0  998.0  2018-01-15 00:00:00\n",
       "25470           3.0  998.0  2018-01-22 00:00:00\n",
       "25471           2.0  998.0  2018-01-29 00:00:00\n",
       "25472           3.0  998.0  2018-02-05 00:00:00\n",
       "25473           3.0  998.0  2018-02-12 00:00:00\n",
       "25474           3.0  998.0  2018-02-19 00:00:00\n",
       "25475           2.0  998.0  2018-02-26 00:00:00\n",
       "25476           2.0  998.0  2018-03-05 00:00:00\n",
       "25477           3.0  998.0  2018-03-12 00:00:00\n",
       "25478           2.0  998.0  2018-03-19 00:00:00\n",
       "25479           2.0  998.0  2018-03-26 00:00:00\n",
       "25480           3.0  998.0  2018-04-02 00:00:00\n",
       "25481           3.0  998.0  2018-04-09 00:00:00\n",
       "25482           3.0  998.0  2018-04-16 00:00:00\n",
       "25483           3.0  998.0  2018-04-30 00:00:00\n",
       "25484           3.0  998.0  2018-05-07 00:00:00\n",
       "25485           3.0  998.0  2018-05-14 00:00:00\n",
       "25486           3.0  998.0  2018-05-21 00:00:00\n",
       "25487           3.0  998.0  2018-05-28 00:00:00"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emails[emails.user == 998]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2018-05-28 00:00:00'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(max(emails[emails.user == 998].week))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2017-12-04 00:00:00'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(emails[emails.user == 998].week)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(max(emails[emails.member == 998].week) -\n",
    "min(emails[emails.member == 998].week)).days/7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24, 3)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emails[emails.user == 998].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_idx = pd.MultiIndex.from_product((set(emails.week),\n",
    "set(emails.user)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_email = emails.set_index(['week', 'member']).\n",
    "reindex(complete_idx, fill_value = 0).\n",
    "reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_email.columns = ['week', 'member', 'EmailsOpened']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## python\n",
    "cutoff_dates = emails.groupby('member').week.\n",
    "agg(['min', 'max']).reset_index)\n",
    "cutoff_dates = cutoff_dates.reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## python\n",
    ">>> for _, row in cutoff_dates.iterrows():\n",
    ">>> member = row['member']\n",
    ">>> start_date = row['min']\n",
    ">>> end_date = row['max']\n",
    ">>> all_email.drop(\n",
    "all_email[all_email.member == member]\n",
    "[all_email.week < start_date].index, inplace=True)\n",
    ">>> all_email.drop(all_email[all_email.member == member]\n",
    "[all_email.week > end_date].index, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## python\n",
    ">>> donations.timestamp = pd.to_datetime(donations.timestamp)\n",
    ">>> donations.set_index('timestamp', inplace = True)\n",
    ">>> agg_don = donations.groupby('member').apply(\n",
    "lambda df: df.amount.resample(\"W-MON\").sum().dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## python\n",
    ">>> for member, member_email in all_email.groupby('member'):\n",
    ">>> member_donations = agg_donations[agg_donations.member\n",
    "== member]\n",
    ">>> member_donations.set_index('timestamp', inplace = True)\n",
    ">>> member_email.set_index ('week', inplace = True)\n",
    "\n",
    ">>> member_email = all_email[all_email.member == member]\n",
    ">>> member_email.sort_values('week').set_index('week')\n",
    ">>> df = pd.merge(member_email, member_donations, how = 'left',\n",
    "left_index = True,\n",
    "right_index = True)\n",
    ">>> df.fillna(0)\n",
    ">>> df['member'] = df.member_x\n",
    ">>> merged_df = merged_df.append(df.reset_index()\n",
    "[['member', 'week', 'emailsOpened',\n",
    "'amount']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## python\n",
    ">>> df = merged_df[merged_df.member == 998]\n",
    ">>> df['target'] = df.amount.shift(1)\n",
    ">>> df = df.fillna(0)\n",
    ">>> df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## python\n",
    ">>> df['dt'] = df.time - df.time.shift(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning Your Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-761fdbb6bded>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-761fdbb6bded>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    Missing data\u001b[0m\n\u001b[1;37m               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Missing data\n",
    "Changing the frequency of a time series (that is, upsampling and\n",
    "downsampling)\n",
    "Smoothing data\n",
    "Addressing seasonality in data\n",
    "Preventing unintentional lookaheads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# a few specific ways to fill in numbers for those missing values:\n",
    "\n",
    "Forward fill\n",
    "\n",
    "Moving average\n",
    "\n",
    "Interpolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LOKAHEAD\n",
    "\n",
    "The term lookahead is used in time series analysis to denote any\n",
    "\n",
    "knowledge of the future. You shouldn’t have such knowledge when\n",
    "\n",
    "designing, training, or evaluating a model. A lookahead is a way, through\n",
    "\n",
    "data, to find out something about the future earlier than you ought to\n",
    "\n",
    "know it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downsampling:\n",
    "\n",
    "downsampling is as simple as selecting\n",
    "\n",
    "out every nth element"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Upsampling:\n",
    "\n",
    "Irregular time series\n",
    "\n",
    "Inputs sampled at different frequencies\n",
    "\n",
    "Knowledge of time series dynamics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smoothing Data\n",
    "\n",
    "Exponential smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Columns = ['Date','Passengers']\n",
    "air = pd.read_csv('BookRepo-master\\Ch02\\data\\AirPassengers.csv', header=None)\n",
    "air.columns = Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Passengers</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1949-01</td>\n",
       "      <td>112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1949-02</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1949-03</td>\n",
       "      <td>132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1949-04</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1949-05</td>\n",
       "      <td>121</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Date  Passengers\n",
       "0  1949-01         112\n",
       "1  1949-02         118\n",
       "2  1949-03         132\n",
       "3  1949-04         129\n",
       "4  1949-05         121"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "air.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pandas' has no attribute 'ewma'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-346b0479064e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mair\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Smooth.5'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mewma\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mair\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m.5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPassengers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mair\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Smooth.9'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mewma\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mair\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m.9\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPassengers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'pandas' has no attribute 'ewma'"
     ]
    }
   ],
   "source": [
    "air['Smooth.5'] = pd.ewma(air, alpha = .5).Passengers\n",
    "air['Smooth.9'] = pd.ewma(air, alpha = .9).Passengers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "air['Smooth.5'] = air.ewm(alpha = .5).mean().Passengers\n",
    "air['Smooth.9'] = air.ewm(alpha = .9).mean().Passengers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Passengers</th>\n",
       "      <th>Smooth.5</th>\n",
       "      <th>Smooth.9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1949-01</td>\n",
       "      <td>112</td>\n",
       "      <td>112.000000</td>\n",
       "      <td>112.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1949-02</td>\n",
       "      <td>118</td>\n",
       "      <td>116.000000</td>\n",
       "      <td>117.454545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1949-03</td>\n",
       "      <td>132</td>\n",
       "      <td>125.142857</td>\n",
       "      <td>130.558559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1949-04</td>\n",
       "      <td>129</td>\n",
       "      <td>127.200000</td>\n",
       "      <td>129.155716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1949-05</td>\n",
       "      <td>121</td>\n",
       "      <td>124.000000</td>\n",
       "      <td>121.815498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1949-06</td>\n",
       "      <td>135</td>\n",
       "      <td>129.587302</td>\n",
       "      <td>133.681562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1949-07</td>\n",
       "      <td>148</td>\n",
       "      <td>138.866142</td>\n",
       "      <td>146.568157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1949-08</td>\n",
       "      <td>148</td>\n",
       "      <td>143.450980</td>\n",
       "      <td>147.856816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1949-09</td>\n",
       "      <td>136</td>\n",
       "      <td>139.718200</td>\n",
       "      <td>137.185682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1949-10</td>\n",
       "      <td>119</td>\n",
       "      <td>129.348974</td>\n",
       "      <td>120.818568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1949-11</td>\n",
       "      <td>104</td>\n",
       "      <td>116.668295</td>\n",
       "      <td>105.681857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Date  Passengers    Smooth.5    Smooth.9\n",
       "0   1949-01         112  112.000000  112.000000\n",
       "1   1949-02         118  116.000000  117.454545\n",
       "2   1949-03         132  125.142857  130.558559\n",
       "3   1949-04         129  127.200000  129.155716\n",
       "4   1949-05         121  124.000000  121.815498\n",
       "5   1949-06         135  129.587302  133.681562\n",
       "6   1949-07         148  138.866142  146.568157\n",
       "7   1949-08         148  143.450980  147.856816\n",
       "8   1949-09         136  139.718200  137.185682\n",
       "9   1949-10         119  129.348974  120.818568\n",
       "10  1949-11         104  116.668295  105.681857"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "air.head(11)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kalman and LOESS incorporate data both earlier and later in\n",
    "\n",
    "time, so if you use these methods keep in mind the leak of information\n",
    "\n",
    "backward in time, as well as the fact that they are usually not appropriate for\n",
    "\n",
    "preparing data to be used in forecasting applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seasonal Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import pytz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2019, 11, 27, 14, 41, 59, 738794)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.datetime.utcnow()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2019, 11, 27, 11, 42, 11, 368460)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2019, 11, 27, 14, 42, 21, 714051, tzinfo=datetime.timezone.utc)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datetime.datetime.now(datetime.timezone.utc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'US/Pacific'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "western = pytz.timezone('US/Pacific')\n",
    "western.zone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_dt = western.localize(datetime.datetime(2018, 5, 15, 12, 34, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2018, 5, 15, 12, 34, tzinfo=<DstTzInfo 'US/Pacific' PDT-1 day, 17:00:00 DST>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loc_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Africa/Abidjan', 'Africa/Accra', 'Africa/Addis_Ababa', 'Africa/Algiers', 'Africa/Asmara', 'Africa/Bamako', 'Africa/Bangui', 'Africa/Banjul', 'Africa/Bissau', 'Africa/Blantyre', 'Africa/Brazzaville', 'Africa/Bujumbura', 'Africa/Cairo', 'Africa/Casablanca', 'Africa/Ceuta', 'Africa/Conakry', 'Africa/Dakar', 'Africa/Dar_es_Salaam', 'Africa/Djibouti', 'Africa/Douala', 'Africa/El_Aaiun', 'Africa/Freetown', 'Africa/Gaborone', 'Africa/Harare', 'Africa/Johannesburg', 'Africa/Juba', 'Africa/Kampala', 'Africa/Khartoum', 'Africa/Kigali', 'Africa/Kinshasa', 'Africa/Lagos', 'Africa/Libreville', 'Africa/Lome', 'Africa/Luanda', 'Africa/Lubumbashi', 'Africa/Lusaka', 'Africa/Malabo', 'Africa/Maputo', 'Africa/Maseru', 'Africa/Mbabane', 'Africa/Mogadishu', 'Africa/Monrovia', 'Africa/Nairobi', 'Africa/Ndjamena', 'Africa/Niamey', 'Africa/Nouakchott', 'Africa/Ouagadougou', 'Africa/Porto-Novo', 'Africa/Sao_Tome', 'Africa/Tripoli', 'Africa/Tunis', 'Africa/Windhoek', 'America/Adak', 'America/Anchorage', 'America/Anguilla', 'America/Antigua', 'America/Araguaina', 'America/Argentina/Buenos_Aires', 'America/Argentina/Catamarca', 'America/Argentina/Cordoba', 'America/Argentina/Jujuy', 'America/Argentina/La_Rioja', 'America/Argentina/Mendoza', 'America/Argentina/Rio_Gallegos', 'America/Argentina/Salta', 'America/Argentina/San_Juan', 'America/Argentina/San_Luis', 'America/Argentina/Tucuman', 'America/Argentina/Ushuaia', 'America/Aruba', 'America/Asuncion', 'America/Atikokan', 'America/Bahia', 'America/Bahia_Banderas', 'America/Barbados', 'America/Belem', 'America/Belize', 'America/Blanc-Sablon', 'America/Boa_Vista', 'America/Bogota', 'America/Boise', 'America/Cambridge_Bay', 'America/Campo_Grande', 'America/Cancun', 'America/Caracas', 'America/Cayenne', 'America/Cayman', 'America/Chicago', 'America/Chihuahua', 'America/Costa_Rica', 'America/Creston', 'America/Cuiaba', 'America/Curacao', 'America/Danmarkshavn', 'America/Dawson', 'America/Dawson_Creek', 'America/Denver', 'America/Detroit', 'America/Dominica', 'America/Edmonton', 'America/Eirunepe', 'America/El_Salvador', 'America/Fort_Nelson', 'America/Fortaleza', 'America/Glace_Bay', 'America/Godthab', 'America/Goose_Bay', 'America/Grand_Turk', 'America/Grenada', 'America/Guadeloupe', 'America/Guatemala', 'America/Guayaquil', 'America/Guyana', 'America/Halifax', 'America/Havana', 'America/Hermosillo', 'America/Indiana/Indianapolis', 'America/Indiana/Knox', 'America/Indiana/Marengo', 'America/Indiana/Petersburg', 'America/Indiana/Tell_City', 'America/Indiana/Vevay', 'America/Indiana/Vincennes', 'America/Indiana/Winamac', 'America/Inuvik', 'America/Iqaluit', 'America/Jamaica', 'America/Juneau', 'America/Kentucky/Louisville', 'America/Kentucky/Monticello', 'America/Kralendijk', 'America/La_Paz', 'America/Lima', 'America/Los_Angeles', 'America/Lower_Princes', 'America/Maceio', 'America/Managua', 'America/Manaus', 'America/Marigot', 'America/Martinique', 'America/Matamoros', 'America/Mazatlan', 'America/Menominee', 'America/Merida', 'America/Metlakatla', 'America/Mexico_City', 'America/Miquelon', 'America/Moncton', 'America/Monterrey', 'America/Montevideo', 'America/Montserrat', 'America/Nassau', 'America/New_York', 'America/Nipigon', 'America/Nome', 'America/Noronha', 'America/North_Dakota/Beulah', 'America/North_Dakota/Center', 'America/North_Dakota/New_Salem', 'America/Ojinaga', 'America/Panama', 'America/Pangnirtung', 'America/Paramaribo', 'America/Phoenix', 'America/Port-au-Prince', 'America/Port_of_Spain', 'America/Porto_Velho', 'America/Puerto_Rico', 'America/Punta_Arenas', 'America/Rainy_River', 'America/Rankin_Inlet', 'America/Recife', 'America/Regina', 'America/Resolute', 'America/Rio_Branco', 'America/Santarem', 'America/Santiago', 'America/Santo_Domingo', 'America/Sao_Paulo', 'America/Scoresbysund', 'America/Sitka', 'America/St_Barthelemy', 'America/St_Johns', 'America/St_Kitts', 'America/St_Lucia', 'America/St_Thomas', 'America/St_Vincent', 'America/Swift_Current', 'America/Tegucigalpa', 'America/Thule', 'America/Thunder_Bay', 'America/Tijuana', 'America/Toronto', 'America/Tortola', 'America/Vancouver', 'America/Whitehorse', 'America/Winnipeg', 'America/Yakutat', 'America/Yellowknife', 'Antarctica/Casey', 'Antarctica/Davis', 'Antarctica/DumontDUrville', 'Antarctica/Macquarie', 'Antarctica/Mawson', 'Antarctica/McMurdo', 'Antarctica/Palmer', 'Antarctica/Rothera', 'Antarctica/Syowa', 'Antarctica/Troll', 'Antarctica/Vostok', 'Arctic/Longyearbyen', 'Asia/Aden', 'Asia/Almaty', 'Asia/Amman', 'Asia/Anadyr', 'Asia/Aqtau', 'Asia/Aqtobe', 'Asia/Ashgabat', 'Asia/Atyrau', 'Asia/Baghdad', 'Asia/Bahrain', 'Asia/Baku', 'Asia/Bangkok', 'Asia/Barnaul', 'Asia/Beirut', 'Asia/Bishkek', 'Asia/Brunei', 'Asia/Chita', 'Asia/Choibalsan', 'Asia/Colombo', 'Asia/Damascus', 'Asia/Dhaka', 'Asia/Dili', 'Asia/Dubai', 'Asia/Dushanbe', 'Asia/Famagusta', 'Asia/Gaza', 'Asia/Hebron', 'Asia/Ho_Chi_Minh', 'Asia/Hong_Kong', 'Asia/Hovd', 'Asia/Irkutsk', 'Asia/Jakarta', 'Asia/Jayapura', 'Asia/Jerusalem', 'Asia/Kabul', 'Asia/Kamchatka', 'Asia/Karachi', 'Asia/Kathmandu', 'Asia/Khandyga', 'Asia/Kolkata', 'Asia/Krasnoyarsk', 'Asia/Kuala_Lumpur', 'Asia/Kuching', 'Asia/Kuwait', 'Asia/Macau', 'Asia/Magadan', 'Asia/Makassar', 'Asia/Manila', 'Asia/Muscat', 'Asia/Nicosia', 'Asia/Novokuznetsk', 'Asia/Novosibirsk', 'Asia/Omsk', 'Asia/Oral', 'Asia/Phnom_Penh', 'Asia/Pontianak', 'Asia/Pyongyang', 'Asia/Qatar', 'Asia/Qostanay', 'Asia/Qyzylorda', 'Asia/Riyadh', 'Asia/Sakhalin', 'Asia/Samarkand', 'Asia/Seoul', 'Asia/Shanghai', 'Asia/Singapore', 'Asia/Srednekolymsk', 'Asia/Taipei', 'Asia/Tashkent', 'Asia/Tbilisi', 'Asia/Tehran', 'Asia/Thimphu', 'Asia/Tokyo', 'Asia/Tomsk', 'Asia/Ulaanbaatar', 'Asia/Urumqi', 'Asia/Ust-Nera', 'Asia/Vientiane', 'Asia/Vladivostok', 'Asia/Yakutsk', 'Asia/Yangon', 'Asia/Yekaterinburg', 'Asia/Yerevan', 'Atlantic/Azores', 'Atlantic/Bermuda', 'Atlantic/Canary', 'Atlantic/Cape_Verde', 'Atlantic/Faroe', 'Atlantic/Madeira', 'Atlantic/Reykjavik', 'Atlantic/South_Georgia', 'Atlantic/St_Helena', 'Atlantic/Stanley', 'Australia/Adelaide', 'Australia/Brisbane', 'Australia/Broken_Hill', 'Australia/Currie', 'Australia/Darwin', 'Australia/Eucla', 'Australia/Hobart', 'Australia/Lindeman', 'Australia/Lord_Howe', 'Australia/Melbourne', 'Australia/Perth', 'Australia/Sydney', 'Canada/Atlantic', 'Canada/Central', 'Canada/Eastern', 'Canada/Mountain', 'Canada/Newfoundland', 'Canada/Pacific', 'Europe/Amsterdam', 'Europe/Andorra', 'Europe/Astrakhan', 'Europe/Athens', 'Europe/Belgrade', 'Europe/Berlin', 'Europe/Bratislava', 'Europe/Brussels', 'Europe/Bucharest', 'Europe/Budapest', 'Europe/Busingen', 'Europe/Chisinau', 'Europe/Copenhagen', 'Europe/Dublin', 'Europe/Gibraltar', 'Europe/Guernsey', 'Europe/Helsinki', 'Europe/Isle_of_Man', 'Europe/Istanbul', 'Europe/Jersey', 'Europe/Kaliningrad', 'Europe/Kiev', 'Europe/Kirov', 'Europe/Lisbon', 'Europe/Ljubljana', 'Europe/London', 'Europe/Luxembourg', 'Europe/Madrid', 'Europe/Malta', 'Europe/Mariehamn', 'Europe/Minsk', 'Europe/Monaco', 'Europe/Moscow', 'Europe/Oslo', 'Europe/Paris', 'Europe/Podgorica', 'Europe/Prague', 'Europe/Riga', 'Europe/Rome', 'Europe/Samara', 'Europe/San_Marino', 'Europe/Sarajevo', 'Europe/Saratov', 'Europe/Simferopol', 'Europe/Skopje', 'Europe/Sofia', 'Europe/Stockholm', 'Europe/Tallinn', 'Europe/Tirane', 'Europe/Ulyanovsk', 'Europe/Uzhgorod', 'Europe/Vaduz', 'Europe/Vatican', 'Europe/Vienna', 'Europe/Vilnius', 'Europe/Volgograd', 'Europe/Warsaw', 'Europe/Zagreb', 'Europe/Zaporozhye', 'Europe/Zurich', 'GMT', 'Indian/Antananarivo', 'Indian/Chagos', 'Indian/Christmas', 'Indian/Cocos', 'Indian/Comoro', 'Indian/Kerguelen', 'Indian/Mahe', 'Indian/Maldives', 'Indian/Mauritius', 'Indian/Mayotte', 'Indian/Reunion', 'Pacific/Apia', 'Pacific/Auckland', 'Pacific/Bougainville', 'Pacific/Chatham', 'Pacific/Chuuk', 'Pacific/Easter', 'Pacific/Efate', 'Pacific/Enderbury', 'Pacific/Fakaofo', 'Pacific/Fiji', 'Pacific/Funafuti', 'Pacific/Galapagos', 'Pacific/Gambier', 'Pacific/Guadalcanal', 'Pacific/Guam', 'Pacific/Honolulu', 'Pacific/Kiritimati', 'Pacific/Kosrae', 'Pacific/Kwajalein', 'Pacific/Majuro', 'Pacific/Marquesas', 'Pacific/Midway', 'Pacific/Nauru', 'Pacific/Niue', 'Pacific/Norfolk', 'Pacific/Noumea', 'Pacific/Pago_Pago', 'Pacific/Palau', 'Pacific/Pitcairn', 'Pacific/Pohnpei', 'Pacific/Port_Moresby', 'Pacific/Rarotonga', 'Pacific/Saipan', 'Pacific/Tahiti', 'Pacific/Tarawa', 'Pacific/Tongatapu', 'Pacific/Wake', 'Pacific/Wallis', 'US/Alaska', 'US/Arizona', 'US/Central', 'US/Eastern', 'US/Hawaii', 'US/Mountain', 'US/Pacific', 'UTC']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytz.common_timezones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Europe/Istanbul']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pytz.country_timezones('tr')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preventing Lookahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nIntentionally introduce a lookahead and see how your model\\nbehaves. Try various degrees of lookahead, so you have an idea how\\nit shifts accuracy. If you have some idea of the accuracy with\\nlookahead, you have an idea of what the ceiling on a real model\\nwithout unfair knowledge of the future will do. Remember that many\\ntime series problems are extremely difficult, so a model with a\\nlookahead may seem great until you realize you are dealing with a\\nhigh-noise/low-signal data set.\\n\\nAdd features slowly, particularly features you might be processing,\\nso that you can look for jumps. One sign of a lookahead is when a\\nparticular feature is unexpectedly good, and there isn’t a very good\\nexplanation. At the top of your explanation list should always be\\n“lookahead.”\\n'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Intentionally introduce a lookahead and see how your model\n",
    "behaves. Try various degrees of lookahead, so you have an idea how\n",
    "it shifts accuracy. If you have some idea of the accuracy with\n",
    "lookahead, you have an idea of what the ceiling on a real model\n",
    "without unfair knowledge of the future will do. Remember that many\n",
    "time series problems are extremely difficult, so a model with a\n",
    "lookahead may seem great until you realize you are dealing with a\n",
    "high-noise/low-signal data set.\n",
    "\n",
    "Add features slowly, particularly features you might be processing,\n",
    "so that you can look for jumps. One sign of a lookahead is when a\n",
    "particular feature is unexpectedly good, and there isn’t a very good\n",
    "explanation. At the top of your explanation list should always be\n",
    "“lookahead.”\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 3. Exploratory Data Analysis for Time Series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Familiar Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nYou will want to address the\\nsame exploratory questions you would ask about any new data set, such as:\\n\\nAre any of the columns strongly correlated with one another?\\n\\nWhat is the overall mean of an interesting variable? What is its\\nvariance?\\n'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "You will want to address the\n",
    "same exploratory questions you would ask about any new data set, such as:\n",
    "\n",
    "Are any of the columns strongly correlated with one another?\n",
    "\n",
    "What is the overall mean of an interesting variable? What is its\n",
    "variance?\n",
    "\n",
    "To answer these, you can use familiar techniques such as plotting, taking\n",
    "summary statistics, applying histograms, and using targeted scatter plots.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nWhat is the range of values you see, and do they vary by time period\\nor some other logical unit of analysis?\\n\\nDoes the data look consistent and uniformly measured, or does it\\nsuggest changes in either measurement or behavior over time?\\n\\n'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "What is the range of values you see, and do they vary by time period\n",
    "or some other logical unit of analysis?\n",
    "\n",
    "Does the data look consistent and uniformly measured, or does it\n",
    "suggest changes in either measurement or behavior over time?\n",
    "\n",
    "To answer these, you can use familiar techniques such as plotting, taking\n",
    "summary statistics, applying histograms, and using targeted scatter plots.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In a time series context, a hist() of the difference of the data is often more\n",
    "interesting than a hist() of the untransformed data\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stationary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Tests for determining whether a process is stationary are called hypothesis\n",
    "tests. The Augmented Dickey–Fuller (ADF) test is the most commonly used\n",
    "metric to assess a time series for stationarity problems.\n",
    "\n",
    "This test posits a null\n",
    "hypothesis that a unit root is present in a time series\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the Kwiatkowski-PhillipsSchmidt-Shin (KPSS) test'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"the Kwiatkowski-PhillipsSchmidt-Shin (KPSS) test\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"A log transformation and a square root\n",
    "transformation are two popular options, particularly in the case of changing\n",
    "variance over time. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USUALLY TWO ASSUMPTIONS FOR FORECAST MODELS, STATIONARITY AND NORMAL DISTRIBUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Normality transformation\n",
    "\n",
    "\"\"\"\n",
    "A common one is the Box Cox transformation, which is\n",
    "implemented in the R forecast package and in scipy.stats in Python\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" \n",
    "Self-Correlation\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Spurious Correlations\n",
    "\n",
    "https://www.tylervigen.com/spurious-correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COINTEGRATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. sımulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## membership status\n",
    "years = ['2014', '2015', '2016', '2017', '2018']\n",
    "memberStatus = ['bronze', 'silver', 'gold', 'inactive']\n",
    "memberYears = np.random.choice(years, 1000,\n",
    "p = [0.1, 0.1, 0.15, 0.30, 0.35])\n",
    "memberStats = np.random.choice(memberStatus, 1000,\n",
    "p = [0.5, 0.3, 0.1, 0.1])\n",
    "yearJoined = pd.DataFrame({'yearJoined': memberYears,\n",
    "'memberStats': memberStats})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## python\n",
    "NUM_EMAILS_SENT_WEEKLY = 3\n",
    "## we define several functions for different patterns\n",
    "def never_opens(period_rng):\n",
    "    return []\n",
    "def constant_open_rate(period_rng):\n",
    "    n, p = NUM_EMAILS_SENT_WEEKLY, np.random.uniform(0, 1)\n",
    "    num_opened = np.random.binomial(n, p, len(period_rng))\n",
    "    return num_opened\n",
    "\n",
    "def increasing_open_rate(period_rng):\n",
    "    return open_rate_with_factor_change(period_rng,\n",
    "    np.random.uniform(1.01,\n",
    "    1.30))\n",
    "def decreasing_open_rate(period_rng):\n",
    "    return open_rate_with_factor_change(period_rng,np.random.uniform(0.5,0.99))\n",
    "\n",
    "def open_rate_with_factor_change(period_rng, fac):\n",
    "    if len(period_rng) < 1 :\n",
    "        return []\n",
    "    times = np.random.randint(0, len(period_rng),\n",
    "    int(0.1 * len(period_rng)))\n",
    "    num_opened = np.zeros(len(period_rng))\n",
    "    for prd in range(0, len(period_rng), 2):\n",
    "        try:\n",
    "            n, p = NUM_EMAILS_SENT_WEEKLY, np.random.uniform(0,1)\n",
    "            num_opened[prd:(prd + 2)] = np.random.binomial(n, p,2)\n",
    "            p = max(min(1, p * fac), 0)\n",
    "        except:\n",
    "            num_opened[prd] = np.random.binomial(n, p, 1)\n",
    "    for t in range(len(times)):\n",
    "            num_opened[times[t]] = 0\n",
    "    return num_opened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## python\n",
    ">>> ## donation behavior\n",
    ">>> def produce_donations(period_rng, member_behavior, num_emails,\n",
    ">>> use_id, member_join_year):\n",
    ">>> donation_amounts = np.array([0, 25, 50, 75, 100, 250, 500,\n",
    ">>> 1000, 1500, 2000])\n",
    ">>> member_has = np.random.choice(donation_amounts)\n",
    ">>> email_fraction = num_emails /\n",
    ">>> (NUM_EMAILS_SENT_WEEKLY * len(period_rng))\n",
    ">>> member_gives = member_has * email_fraction\n",
    ">>> member_gives_idx = np.where(member_gives\n",
    ">>> >= donation_amounts)[0][-1]\n",
    ">>> member_gives_idx = max(min(member_gives_idx,\n",
    ">>> len(donation_amounts) - 2),\n",
    ">>> 1)\n",
    ">>> num_times_gave = np.random.poisson(2) *\n",
    ">>> (2018 - member_join_year)\n",
    ">>> times = np.random.randint(0, len(period_rng), num_times_gave)\n",
    ">>> dons = pd.DataFrame({'member' : [],\n",
    ">>> 'amount' : [],\n",
    ">>> 'timestamp': []})\n",
    ">>> for n in range(num_times_gave):\n",
    ">>> donation = donation_amounts[member_gives_idx\n",
    ">>> + np.random.binomial(1, .3)]\n",
    ">>> ts = str(period_rng[times[n]].start_time\n",
    ">>> + random_weekly_time_delta())\n",
    ">>> dons = dons.append(pd.DataFrame(\n",
    ">>> {'member' : [use_id],\n",
    ">>> 'amount' : [donation],\n",
    ">>> 'timestamp': [ts]}))\n",
    ">>>\n",
    ">>> if dons.shape[0] > 0:\n",
    ">>> dons = dons[dons.amount != 0]\n",
    ">>> ## we don't report zero donation events as this would not\n",
    ">>> ## be recorded in a real world database\n",
    ">>>\n",
    ">>> return dons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## python\n",
    ">>> def random_weekly_time_delta():\n",
    ">>> days_of_week = [d for d in range(7)]\n",
    ">>> hours_of_day = [h for h in range(11, 23)]\n",
    ">>> minute_of_hour = [m for m in range(60)]\n",
    ">>> second_of_minute = [s for s in range(60)]\n",
    ">>> return pd.Timedelta(str(np.random.choice(days_of_week))\n",
    ">>> + \" days\" ) +\n",
    ">>> pd.Timedelta(str(np.random.choice(hours_of_day))\n",
    ">>> + \" hours\" ) +\n",
    ">>> pd.Timedelta(str(np.random.choice(minute_of_hour))\n",
    ">>> + \" minutes\") +\n",
    ">>> pd.Timedelta(str(np.random.choice(second_of_minute))\n",
    ">>> + \" seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## python\n",
    ">>> behaviors = [never_opens,\n",
    ">>> constant_open_rate,\n",
    ">>> increasing_open_rate,\n",
    ">>> decreasing_open_rate]\n",
    ">>> member_behaviors = np.random.choice(behaviors, 1000,\n",
    ">>> [0.2, 0.5, 0.1, 0.2])\n",
    ">>> rng = pd.period_range('2015-02-14', '2018-06-01', freq = 'W')\n",
    ">>> emails = pd.DataFrame({'member' : [],\n",
    ">>> 'week' : [],\n",
    ">>> 'emailsOpened': []})\n",
    ">>> donations = pd.DataFrame({'member' : [],\n",
    ">>> 'amount' : [],\n",
    ">>> 'timestamp': []})\n",
    ">>> for idx in range(yearJoined.shape[0]):\n",
    ">>> ## randomly generate the date when a member would have joined\n",
    ">>> join_date = pd.Timestamp(yearJoined.iloc[idx].yearJoined) +\n",
    ">>> pd.Timedelta(str(np.random.randint(0, 365)) +\n",
    ">>> ' days')\n",
    ">>> join_date = min(join_date, pd.Timestamp('2018-06-01'))\n",
    ">>>\n",
    ">>> ## member should not have action timestamps before joining\n",
    ">>> member_rng = rng[rng > join_date]\n",
    ">>>\n",
    ">>> if len(member_rng) < 1:\n",
    ">>> continue\n",
    ">>>\n",
    ">>> info = member_behaviors[idx](member_rng)\n",
    ">>> if len(info) == len(member_rng):\n",
    ">>> emails = emails.append(pd.DataFrame(\n",
    ">>> {'member': [idx] * len(info),\n",
    ">>> 'week': [str(r.start_time) for r in member_rng],\n",
    ">>> 'emailsOpened': info}))\n",
    ">>> donations = donations.append(\n",
    ">>> produce_donations(member_rng, member_behaviors[idx],\n",
    ">>> sum(info), idx, join_date.year))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## python\n",
    ">>> df.set_index(pd.to_datetime(df.timestamp), inplace = True)\n",
    ">>> df.sort_index(inplace = True)\n",
    ">>> df.groupby(pd.Grouper(freq='M')).amount.sum().plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PAGE 159 -- Building a Simulation Universe That Runs Itself\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## python\n",
    ">>> import numpy as np\n",
    ">>> def taxi_id_number(num_taxis):\n",
    ">>> arr = np.arange(num_taxis)\n",
    ">>> np.random.shuffle(arr)\n",
    ">>> for i in range(num_taxis):\n",
    ">>> yield arr[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## python\n",
    ">>> ids = taxi_id_number(10)\n",
    ">>> print(next(ids))\n",
    ">>> print(next(ids))\n",
    ">>> print(next(ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## python\n",
    ">>> def shift_info():\n",
    ">>> start_times_and_freqs = [(0, 8), (8, 30), (16, 15)]\n",
    ">>> indices = np.arange(len(start_times_and_freqs))\n",
    ">>> while True:\n",
    ">>> idx = np.random.choice(indices, p = [0.25, 0.5, 0.25])\n",
    ">>> start = start_times_and_freqs[idx]\n",
    ">>> yield (start[0], start[0] + 7.5, start[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## python\n",
    ">>> def taxi_process(taxi_id_generator, shift_info_generator):\n",
    ">>> taxi_id = next(taxi_id_generator)\n",
    ">>> shift_start, shift_end, shift_mean_trips =\n",
    ">>> next(shift_info_generator)\n",
    ">>> actual_trips = round(np.random.normal(loc = shift_mean_trips,\n",
    ">>> scale = 2))\n",
    ">>> average_trip_time = 6.5 / shift_mean_trips * 60\n",
    ">>> # convert mean trip time to minutes\n",
    ">>> between_events_time = 1.0 / (shift_mean_trips - 1) * 60\n",
    ">>> # this is an efficient city where cabs are seldom unused\n",
    ">>> time = shift_start\n",
    ">>> yield TimePoint(taxi_id, 'start shift', time)\n",
    ">>> deltaT = np.random.poisson(between_events_time) / 60\n",
    ">>> time += deltaT\n",
    ">>> for i in range(actual_trips):\n",
    ">>> yield TimePoint(taxi_id, 'pick up ', time)\n",
    ">>> deltaT = np.random.poisson(average_trip_time) / 60\n",
    ">>> time += deltaT\n",
    ">>> yield TimePoint(taxi_id, 'drop off ', time)\n",
    ">>> deltaT = np.random.poisson(between_events_time) / 60\n",
    ">>> time += deltaT\n",
    ">>> deltaT = np.random.poisson(between_events_time) / 60\n",
    ">>> time += deltaT\n",
    ">>> yield TimePoint(taxi_id, 'end shift ', time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## python\n",
    ">>> from dataclasses import dataclass\n",
    ">>> @dataclass\n",
    ">>> class TimePoint:\n",
    ">>> taxi_id: int\n",
    ">>> name: str\n",
    ">>> time: float\n",
    ">>> def __lt__(self, other):\n",
    ">>> return self.time < other.time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## python\n",
    ">>> import queue\n",
    ">>> class Simulator:\n",
    ">>> def __init__(self, num_taxis):\n",
    ">>> self._time_points = queue.PriorityQueue()\n",
    ">>> taxi_id_generator = taxi_id_number(num_taxis)\n",
    ">>> shift_info_generator = shift_info()\n",
    ">>> self._taxis = [taxi_process(taxi_id_generator,\n",
    ">>> shift_info_generator) for\n",
    ">>> i in range(num_taxis)]\n",
    ">>> self._prepare_run()\n",
    ">>> def _prepare_run(self):\n",
    ">>> for t in self._taxis:\n",
    ">>> while True:\n",
    ">>> try:\n",
    ">>> e = next(t)\n",
    ">>> self._time_points.put(e)\n",
    ">>> except:\n",
    ">>> break\n",
    ">>> def run(self):\n",
    ">>> sim_time = 0\n",
    ">>> while sim_time < 24:\n",
    ">>> if self._time_points.empty():\n",
    ">>> break\n",
    ">>> p = self._time_points.get()\n",
    ">>> sim_time = p.time\n",
    ">>> print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## python\n",
    ">>> sim = Simulator(1000)\n",
    ">>> sim.run()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 165 - A Physics Simulation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## python\n",
    ">>> ### CONFIGURATION\n",
    ">>> ## physical layout\n",
    ">>> N = 5 # width of lattice\n",
    ">>> M = 5 # height of lattice\n",
    ">>> ## temperature settings\n",
    ">>> temperature = 0.5\n",
    ">>> BETA = 1 / temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    ">>> def initRandState(N, M):\n",
    ">>> block = np.random.choice([-1, 1], size = (N, M))\n",
    ">>> return block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## python\n",
    ">>> def costForCenterState(state, i, j, n, m):\n",
    ">>> centerS = state[i, j]\n",
    ">>> neighbors = [((i + 1) % n, j), ((i - 1) % n, j),\n",
    ">>> (i, (j + 1) % m), (i, (j - 1) % m)]\n",
    ">>> ## notice the % n because we impose periodic boundary cond\n",
    ">>> ## ignore this if it doesn't make sense - it's merely a\n",
    ">>> ## physical constraint on the system saying 2D system is like\n",
    ">>> ## the surface of a donut\n",
    ">>> interactionE = [state[x, y] * centerS for (x, y) in neighbors]\n",
    ">>> return np.sum(interactionE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## python\n",
    ">>> def magnetizationForState(state):\n",
    ">>> return np.sum(state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## python\n",
    ">>> def mcmcAdjust(state):\n",
    ">>> n = state.shape[0]\n",
    ">>> m = state.shape[1]\n",
    ">>> x, y = np.random.randint(0, n), np.random.randint(0, m)\n",
    ">>> centerS = state[x, y]\n",
    ">>> cost = costForCenterState(state, x, y, n, m)\n",
    ">>> if cost < 0:\n",
    ">>> centerS *= -1\n",
    ">>> elif np.random.random() < np.exp(-cost * BETA):\n",
    ">>> centerS *= -1\n",
    ">>> state[x, y] = centerS\n",
    ">>> return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## python\n",
    ">>> def runState(state, n_steps, snapsteps = None):\n",
    ">>> if snapsteps is None:\n",
    ">>> snapsteps = np.linspace(0, n_steps, num = round(n_steps / (M * N *\n",
    "100)),\n",
    ">>> dtype = np.int32)\n",
    ">>> saved_states = []\n",
    ">>> sp = 0\n",
    ">>> magnet_hist = []\n",
    ">>> for i in range(n_steps):\n",
    ">>> state = mcmcAdjust(state)\n",
    ">>> magnet_hist.append(magnetizationForState(state))\n",
    ">>> if sp < len(snapsteps) and i == snapsteps[sp]:\n",
    ">>> saved_states.append(np.copy(state))\n",
    ">>> sp += 1\n",
    ">>> return state, saved_states, magnet_hist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## python\n",
    ">>> ### RUN A SIMULATION\n",
    ">>> init_state = initRandState(N, M)\n",
    ">>> print(init_state)\n",
    ">>> final_state = runState(np.copy(init_state), 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## python\n",
    ">>> we collect each time series as a separate element in results list\n",
    ">>> results = []\n",
    ">>> for i in range(100):\n",
    ">>> init_state = initRandState(N, M)\n",
    ">>> final_state, states, magnet_hist = runState(init_state, 1000)\n",
    ">>> results.append(magnet_hist)\n",
    ">>>\n",
    ">>> ## we plot each curve with some transparency so we can see\n",
    ">>> ## curves that overlap one another\n",
    ">>> for mh in results:\n",
    ">>> plt.plot(mh,'r', alpha=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chapter 5. Storing Temporal Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SQL, NOSQL, Flat File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Time series–specific databases and related monitoring tools\n",
    "\n",
    "InfluxDB, Prometheus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6. Statistical Models for Time Series "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"a linear regression assumes you have independently and identically distributed (iid) data\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ACF\n",
    "\n",
    "Ljung-Box test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
